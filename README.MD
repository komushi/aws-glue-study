https://s3.console.aws.amazon.com/s3/buckets/crawler-public-ap-northeast-1/flight/

s3://crawler-public-ap-northeast-1/flight/2016/csv/


Steps:
# 1. Prepare environment

## 1-0. Fetch source code
```
git clone git@github.com:komushi/aws-glue-study.git
```
## 1-1. Create S3 bucket and upload CloudFormation configuration files
Login to AWS management console then:

* Service -> S3 -> Create Bucket
* Create a folder named as 'cfn': 
```
<your_workshop_bucket>/
  +-- cfn
```
* Upload "iam.yaml" / "sg.yaml" / "vpc.yaml" files under "cfn" folder under repository cloned in 1-0
* Copy full path of each files, saving for later use

## 1-2. Edit master.yaml file
* Open "master.yaml" under "cfn" with proper editor
* Replace URL links ended with "iam.yaml" / "sg.yaml" / "vpc.yaml" with their couterparts copied above and save

## 1-3. Use CloudFormation to prepare environment
* Service -> CloudFormation -> Create new task
* Select template -> Upload template to Amazon S3
* Browse and select "master.yaml", which is just edited and saved.
* Stack name: aws-glue-study
* Input proper UserName and UserPassword (Blank value will cause creation failure!!)
* Check on: I acknowledge that AWS CloudFormation might create IAM resources with custom names.
* Wait until status changed to "CREATE_COMPLETE"

**You will see stacks like:**
```
aws-glue-study-SecurityGroupStack-XXXXXXXXXXXXX [NESTED]
aws-glue-study-IAMStack-MBEUULSULYPQ [NESTED]
aws-glue-study-VPCStack-GR86W4ABT8H1 [NESTED]
aws-glue-study
```

# 2. Play with the original data

## 2-1. Crawl the original data by creating a table - flight_data.csv - by the AWS Glue Crawlers GUI
* Service -> AWS Glue -> Crawlers -> Add Crawler
* Crawler name: crawl-public-flight-2016-csv
* Data store: s3://crawler-public-ap-northeast-1/flight/2016/csv/
* Choose an existing IAM role: AWSGlueServiceRole created by CloudFormation:
```
aws-glue-study-IAMStack-XXXXXX-AWSGlueServiceRole-XXXXXXXXXXXX
```
* Frequency: Run on demand
* Output Database: flight_data
* Run script and wait until finish

## 2-2. Query the data at Athena
* Service -> Athena -> Execute query

```
select count(1) from csv
```

# 3. Import into private s3 bucket as csv

## 3-0. Prepare Data sink by creating folders

**Under bucket created in 1-1, create two more folders as below**
```
<your_workshop_bucket>/
  +-- cfn
  +-- scripts    # new
  +-- data       # new
```

## 3-1. Run ETL Job into private csv data on S3 by the AWS Glue Jobs GUI

* Service -> AWS Glue -> Jobs -> Add job
* Name: job_import_csv
* IAM Role: AWSGlueServiceRole created by CloudFormation:
```
aws-glue-study-IAMStack-XXXXXX-AWSGlueServiceRole-XXXXXXXXXXXX
```
* Script: Use the proposed script by AWS Glue:
```
  ◉ A proposed script generated by AWS Glue
  ◉ Python
```
* Script file name: job_import_csv
* S3 path where the script is stored

```
  s3://<your_workshop_bucket>/scripts               # Created at 3-0
```
* Data source: csv - create at Step 2-1
* Data target: Create a gzip-compressed CSV table in your own S3 table

```
 ◉ Create tables in your data target
    Data store: Amazon S3
    Format: CSV
    Compression type: gzip
    Target path: s3://<your_workshop_bucket>/data/   # Created at 3-0
```
* Save job and edit scripts
* Run job

## 3-2. Crawl to create a table - flight_data.private_csv
* Crawlers -> Add crawler
* Crawler name: crawl-private-flight-2016-csv
* Data store: choose the folder specified at 3-1 as data target

```
Data store: S3
Crawl data in
  ◉ Specified path in my account
  Include path: s3://<your_workshop_bucket>/data/    # data generated at 3-1
```
* Choose an existing IAM role: AWSGlueServiceRole created by CloudFormation
```
aws-glue-study-IAMStack-XXXXXX-AWSGlueServiceRole-XXXXXXXXXXXX
```
* Frequency: Run on demand
* Output Database: flight_data <- Create the database if not created yet
* Prefix: private_
* Finish and run it now, wait until finish

## 3-3. Query the data

* Service -> Athena -> Query editor

```
select count(1) from private_data
```

# 4. ETL to Parquet

## 4-0. Prepare parquet folders
* Service -> S3 -> <your_workshop_bucket> -> Create folder -> flight_parquet

```
<your_workshop_bucket>/
  +-- cfn
  +-- scripts
  +-- data
  +-- flight_parquet    # new
```

## 4-1. Run ETL Job to convert flight_data.flight_csv into private parquet data on S3 - by glue crawler GUI, only data for January
* Service -> AWS Glue -> ETL -> Jobs -> Add job
* Name: conver-to-parquet
* IAM Role: AWSGlueServiceRole created by CloudFormation:
```
aws-glue-study-IAMStack-XXXXXX-AWSGlueServiceRole-XXXXXXXXXXXX
```
* Script: Use the proposed script by AWS Glue:
```
This job runs
  ◉ A proposed script generated by AWS Glue
  ◉ Python
```
* Script file name: conver-to-parquet
* Store script in the "scripts" folder that is created at 3-0:
```
S3 path where the script is stored:
s3://<your_workshop_bucket>/scripts/
```
* Data source: private_data:
```
 ◉ private_data
```
* Data target: parquet in S3:
```
Data store: Amazon S3
Format: Parquet
Target path: s3://<your_workshop_bucket>/flight_parquet/
```
* Save job and run it until finish

## 4-2. Crawl to create a table - flight_data.flight_parquet
* Crawlers -> Add crawler
* Crawler name: crawl-private-parquet
* Data store: S3
* Crawl data generated at 4-1: 
```
Crawl data in:
◉ Specified path in my account
  Include path: s3://<your_workshop_bucket>/flight_parquet/
```
* IAM Role: AWSGlueServiceRole created by CloudFormation:
```
aws-glue-study-IAMStack-XXXXXX-AWSGlueServiceRole-XXXXXXXXXXXX
```
* Frequency: Run on demand
* Database: flight_data
* Finish and run it now, wait until finish

## 4-3. Query the data
* Service -> Athena -> Execute query

```
select count(1) from flight_parquet
```

# 5. Setup managed Zeppelin notebook envrironment

## 5-0. Check the existing development endpoint which was created during Cfn deployment
* Service -> AWS Glue -> Dev endpoints -> myDevEndpoint

## 5-1. Rotate SSH key and download new private key
* Dev endpoints -> Action -> Rotate SSH Key
* Download a private key to your local computer as "private-key.pem"

## 5-2. Create Zeppelin notebook server
* Dev endpoints -> Action -> Create notebook server
* Input notebook parameters as below:
```
  CloudFormation stack name: aws-glue-[your-name-and-date]
  IAM Role: choose the one which name includes "AWSGlueNotebookRule"
  EC2 key pair: use any of existing or just create a new one
  SSH private key: press Upload button to choose the private-key.pem
  Notebook username: admin
  Notebook password: type as you want
```
* Leave all other parameters as default and press Finish
* Service -> CloudFormation -> Stack
* Check if the Zeppelin notebook server is being created, and wait until finished (it may takes a few minutes)

## Connect to the Zeppelin
* Service -> AWS Glue -> Dev endpoints -> myDevEndpoint
* Check if your new instance is now listed under Notebook servers pane
* Click Notebook Server URL to connect to Zeppelin notebook server via Web browser

# 6. Develop custom ETL script using Zeppelin notebook

